{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"jumbotron\">\n",
    "  <h1 class=\"display-3\">Credit Risk Case Study in Python</h1>\n",
    "  <p>Using Logistic Regression</p>\n",
    "  \n",
    "<hr>\n",
    "<h3> What is Credit risk? </h3>\n",
    "Credit risk refers to the risk that a borrower may not repay a loan and that the lender may lose the principal of the loan or the interest associated with it. In Banking sector this is an important factor to be considered before approving the loan of an applicant.\n",
    "\n",
    "<h3> How Is Credit Risk Assessed? </h3>\n",
    "Credit risks are calculated based on the borrowers' overall ability to repay. To assess credit risk on a consumer loan, lenders look at the five C's: an applicant's credit history, his capacity to repay, his capital, the loan's conditions and associated collateral.\n",
    "\n",
    "<h3> Problem </h3>\n",
    "To automate the loan eligibility process based on the customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, we given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers. Here they have provided a partial data set.\n",
    "\n",
    "<h3> Data Variables and Description </h3>\n",
    "<table>\n",
    " <tr> <th align=\"left\">Variable</th> <th align=\"left\">Description</th> </tr>\n",
    " <tr> <td>Loan_ID</td> <td>Unique Loan ID</td> </tr>\n",
    " <tr> <td>Gender</td> <td>Male/ Female</td> </tr>\n",
    " <tr> <td>Married</td> <td>Applicant married (Y/N)</td> </tr>\n",
    " <tr> <td>Dependents</td> <td>Number of dependents</td> </tr>\n",
    " <tr> <td>Education</td> <td>Applicant Education (Graduate/ Under Graduate)</td> </tr>\n",
    " <tr> <td>Self_Employed</td> <td>Self employed (Y/N)</td> </tr>\n",
    " <tr> <td>ApplicantIncome</td> <td>Applicant income</td> </tr>\n",
    " <tr> <td>CoapplicantIncome</td> <td>Coapplicant income</td> </tr>\n",
    " <tr> <td>LoanAmount</td> <td>Loan amount in thousands</td> </tr>\n",
    " <tr> <td>Loan_Amount_Term</td> <td>Term of loan in months</td> </tr>\n",
    " <tr> <td>Credit_History</td> <td>credit history meets guidelines</td> </tr>\n",
    " <tr> <td>Property_Area</td> <td>Urban/ Semi Urban/ Rural</td> </tr>\t\n",
    " <tr> <td>Loan_Status</td> <td>Loan approved (Y/N)</td> </tr>\t\t\t\n",
    "</table>\n",
    "\n",
    "<h3> Dataset File Given </h3>\n",
    "* Credit_Risk_Train_Data\n",
    "* Credit_Risk_Test_Data\n",
    "  \n",
    " \n",
    " <h2><span class=\"label label-info\">Index</span></h2> <br>\n",
    " &#x25FE; [Importing Datasets](#ImportingDatasets) <br>\n",
    " &#x25FE; [Finding NULL Values](#FindNULLVal) <br>\n",
    " &#x25FE; [Counting Levels in Datasets](#CountDSLevels) <br>\n",
    " &#x25FE; [Treating NULL Values & Converting Variables into 0's and 1's](#TreatNULLVal) <br>\n",
    " &#x25FE; [Plot and Graphs](#Plots) <br>\n",
    " &#x25FE; [Logistic Regression](#logreg) <br>\n",
    " &#x25FE; [Classification Report using Stats Model](#classreport_statsmod) <br>\n",
    " &#x25FE; [Classification Report using Sci Kit Learn](#classreport_scikit) <br>\n",
    " &#x25FE; [Model Performance Evaluation](#ModPerformEva) <br>\n",
    " &#x25FE; [Confusion Matrix](#ConfMat) <br>\n",
    " &#x25FE; [Adjusting the Classification Threshold](#AdjClassThershold) <br>\n",
    " &#x25FE; [ Decreasing the Threshold [Optional]](#DecThreshold) <br>\n",
    " &#x25FE; [ROC Curves and Area Under the Curve (AUC)](#rocauc) <br>\n",
    " &#x25FE; [Exporting predicted values in Validate Dataset File](#ExportVals) <br>\n",
    " &#x25FE; [Conclusion](#Conclusion) <br>\n",
    "</div>\n",
    "<a id=\"head\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ImportingDatasets\"></a>\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:#000000\">\n",
    "# Importing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"label label-default\">Importing Traning Dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Train Datset\n",
    "train_data = pd.read_csv(\"F:/Lectures/Data Science/iMarticus/Python/Scripts/iMarticus-Projects/Datasets/Credit_Risk_Train_Data.csv\")\n",
    "train_data = pd.DataFrame(train_data)\n",
    "train_data.shape # Shape gives you total number of observations and variables present in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 614 entries, 0 to 613\n",
      "Data columns (total 13 columns):\n",
      "Loan_ID              614 non-null object\n",
      "Gender               601 non-null object\n",
      "Married              611 non-null object\n",
      "Dependents           599 non-null object\n",
      "Education            614 non-null object\n",
      "Self_Employed        582 non-null object\n",
      "ApplicantIncome      614 non-null int64\n",
      "CoapplicantIncome    614 non-null float64\n",
      "LoanAmount           592 non-null float64\n",
      "Loan_Amount_Term     600 non-null float64\n",
      "Credit_History       564 non-null float64\n",
      "Property_Area        614 non-null object\n",
      "Loan_Status          614 non-null object\n",
      "dtypes: float64(4), int64(1), object(8)\n",
      "memory usage: 62.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married Dependents Education Self_Employed  \\\n",
       "0  LP001002   Male      No          0  Graduate            No   \n",
       "1  LP001003   Male     Yes          1  Graduate            No   \n",
       "2  LP001005   Male     Yes          0  Graduate           Yes   \n",
       "\n",
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5849                0.0         NaN             360.0   \n",
       "1             4583             1508.0       128.0             360.0   \n",
       "2             3000                0.0        66.0             360.0   \n",
       "\n",
       "   Credit_History Property_Area Loan_Status  \n",
       "0             1.0         Urban           Y  \n",
       "1             1.0         Rural           N  \n",
       "2             1.0         Urban           Y  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.info() # Info gives you name of each variable with the data type associated with it.\n",
    "train_data.head(3) # Head gives you first few rows of the dataset. (3 is the nummber of rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"label label-default\">Importing Testing Dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(367, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Test Datset\n",
    "test_data = pd.read_csv(\"F:/Lectures/Data Science/iMarticus/Python/Scripts/iMarticus-Projects/Datasets/Credit_Risk_Test_Data.csv\")\n",
    "test_data = pd.DataFrame(test_data)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 367 entries, 0 to 366\n",
      "Data columns (total 13 columns):\n",
      "Loan_ID              367 non-null object\n",
      "Gender               356 non-null object\n",
      "Married              367 non-null object\n",
      "Dependents           357 non-null object\n",
      "Education            367 non-null object\n",
      "Self_Employed        344 non-null object\n",
      "ApplicantIncome      367 non-null int64\n",
      "CoapplicantIncome    367 non-null int64\n",
      "LoanAmount           362 non-null float64\n",
      "Loan_Amount_Term     361 non-null float64\n",
      "Credit_History       338 non-null float64\n",
      "Property_Area        367 non-null object\n",
      "outcome              367 non-null object\n",
      "dtypes: float64(3), int64(2), object(8)\n",
      "memory usage: 37.4+ KB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001015</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5720</td>\n",
       "      <td>0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001022</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>3076</td>\n",
       "      <td>1500</td>\n",
       "      <td>126.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001031</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5000</td>\n",
       "      <td>1800</td>\n",
       "      <td>208.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married Dependents Education Self_Employed  \\\n",
       "0  LP001015   Male     Yes          0  Graduate            No   \n",
       "1  LP001022   Male     Yes          1  Graduate            No   \n",
       "2  LP001031   Male     Yes          2  Graduate            No   \n",
       "\n",
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5720                  0       110.0             360.0   \n",
       "1             3076               1500       126.0             360.0   \n",
       "2             5000               1800       208.0             360.0   \n",
       "\n",
       "   Credit_History Property_Area outcome  \n",
       "0             1.0         Urban       Y  \n",
       "1             1.0         Urban       Y  \n",
       "2             1.0         Urban       Y  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"FindNULLVal\"></a>\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:#000000\">\n",
    "# Finding NULL Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loan_ID               0\n",
       "Gender               13\n",
       "Married               3\n",
       "Dependents           15\n",
       "Education             0\n",
       "Self_Employed        32\n",
       "ApplicantIncome       0\n",
       "CoapplicantIncome     0\n",
       "LoanAmount           22\n",
       "Loan_Amount_Term     14\n",
       "Credit_History       50\n",
       "Property_Area         0\n",
       "Loan_Status           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum() # Gives Variable wise NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum().sum() #136 # Gives total number of NaN values in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loan_ID               0\n",
       "Gender               11\n",
       "Married               0\n",
       "Dependents           10\n",
       "Education             0\n",
       "Self_Employed        23\n",
       "ApplicantIncome       0\n",
       "CoapplicantIncome     0\n",
       "LoanAmount            5\n",
       "Loan_Amount_Term      6\n",
       "Credit_History       29\n",
       "Property_Area         0\n",
       "outcome               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.isnull().sum().sum() #84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"CountDSLevels\"></a>\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:#000000\">\n",
    "# Counting Levels in Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the levels of variables in dataset is important because it shows the distribution of elements in categorical variable. In addation to that it also shows the wrongly entred elements. Like in case of **Dependents** variable (3+ = 51)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"label label-default\">For Traning Dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.Gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.Married.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.Dependents.value_counts()  # 3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.Education.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.Self_Employed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.LoanAmount.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.Loan_Amount_Term.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.Credit_History.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.Property_Area.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.Loan_Status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"label label-default\">For Testing Dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.Gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.Married.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.Dependents.value_counts()  # 3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.Education.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.Self_Employed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.LoanAmount.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.Loan_Amount_Term.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.Credit_History.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.Property_Area.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"TreatNULLVal\"></a>\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:#000000\">\n",
    "# Treating NULL Values and Converting Variables into 0's and 1's \n",
    "\n",
    "### Key\n",
    "\n",
    "* **Gender**          : Male=1 | Female=0\n",
    "* **Married** \t      : Yes=1\t| No=0\n",
    "* **Dependents**      : 0,1,2,3\n",
    "* **Education**\t  \t  : Graduate=1 | Not Graduate=0\n",
    "* **Self_Employed**   : Yes=1 | No=0\n",
    "* **Credit_History**  : 0 and 1\n",
    "* **Property_Area**   : Urban=1 | Rural=2 | SemiUrban=3\n",
    "* **Loan_Status**\t  : Yes=1\t| No=0\n",
    "\n",
    "### Categorical Encoding\n",
    "* <p>The idea is to convert the all categorical variable into 0s, 1s, 2s, etc and into numeric integers. Though Logistics Regression is roboust to handle categorical variable but it is always a good practice to convert things into numerical values because its all about mathematical calculations.</p>\n",
    "* <p>To do this we have LabelEncoder package from SciKit learn that encodes the labels with value between **0 and n_classes-1**. Though there are several other [methods](http://pbpython.com/categorical-encoding.html)  to do this. [Label Encoder Help](https://chrisalbon.com/machine-learning/convert_pandas_categorical_column_into_integers_for_scikit-learn.html) <br></p>\n",
    "* <p>The **fit_transform(y)** function fit label encoder (male/female) and return encoded labels (1/0) </p>\n",
    "* <p>By default the fit_transform() function will encode the NaN values also. So, we have to take care of them in best possible way.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the LabelEncoder Libraries\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "number = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"label label-default\">For Traning Dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['Gender'] = number.fit_transform(train_data['Gender'].astype('str')) # This makes three levels 0,1, NaN = 2 \n",
    "avgnum = np.round(np.mean(train_data['Gender'])) # So, we have to convert '2' into either 1 or 0 by taking the mean.\n",
    "train_data['Gender'].replace(2,avgnum,inplace=True) # This replaces '2' with Rounded Avg Value\n",
    "\n",
    "train_data['Married'] = number.fit_transform(train_data['Married'].astype('str'))\n",
    "avgnum = np.round(np.mean(train_data['Married']))\n",
    "train_data['Married'].replace(2,avgnum,inplace=True)\n",
    "\n",
    "train_data['Dependents'] = number.fit_transform(train_data['Dependents'].astype('str')) # Creates 4 levels\n",
    "train_data['Dependents'].replace(4,3,inplace=True) # The 4th level is converted into 3\n",
    "\n",
    "train_data['Education'] = number.fit_transform(train_data['Education'].astype('str'))\n",
    "\n",
    "train_data['Self_Employed'] = number.fit_transform(train_data['Self_Employed'].astype('str'))\n",
    "avgnum = np.round(np.mean(train_data['Self_Employed']))\n",
    "train_data['Self_Employed'].replace(2,avgnum,inplace=True)\n",
    "\n",
    "avgnum = np.round(np.mean(train_data['Loan_Amount_Term'])) # It gives 342 which is closer to 360\n",
    "train_data.Loan_Amount_Term.fillna(360 ,inplace = True) # So, filled NA values with 360\n",
    "\n",
    "avgnum = np.round(np.mean(train_data['LoanAmount'])) # It gives 146 \n",
    "train_data.LoanAmount.fillna(146 ,inplace = True) # So, filled NA values with 146\n",
    "\n",
    "avgnum = np.round(np.mean(train_data['Credit_History'])) # It gives 1\n",
    "train_data.Credit_History.fillna(1 ,inplace = True) # So, filled NA values with 1\n",
    "\n",
    "train_data['Property_Area'] = number.fit_transform(train_data['Property_Area'].astype('str'))\n",
    "train_data['Loan_Status'] = number.fit_transform(train_data['Loan_Status'].astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"label label-default\">For Testing Dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['Gender'] = number.fit_transform(test_data['Gender'].astype('str'))  \n",
    "avgnum = np.round(np.mean(test_data['Gender'])) \n",
    "test_data['Gender'].replace(2,avgnum,inplace=True) \n",
    "\n",
    "test_data['Married'] = number.fit_transform(test_data['Married'].astype('str'))\n",
    "avgnum = np.round(np.mean(test_data['Married']))\n",
    "test_data['Married'].replace(2,avgnum,inplace=True)\n",
    "\n",
    "test_data['Dependents'] = number.fit_transform(test_data['Dependents'].astype('str')) # Creates 4 levels\n",
    "test_data['Dependents'].replace(4,3,inplace=True) # The 4th level is converted into 3\n",
    "\n",
    "test_data['Education'] = number.fit_transform(test_data['Education'].astype('str'))\n",
    "\n",
    "test_data['Self_Employed'] = number.fit_transform(test_data['Self_Employed'].astype('str'))\n",
    "avgnum = np.round(np.mean(test_data['Self_Employed']))\n",
    "test_data['Self_Employed'].replace(2,avgnum,inplace=True)\n",
    "\n",
    "avgnum = np.round(np.mean(test_data['Loan_Amount_Term'])) # It gives 343 which is closer to 360\n",
    "test_data.Loan_Amount_Term.fillna(360 ,inplace = True) # So, filled NA values with 360\n",
    "\n",
    "avgnum = np.round(np.mean(test_data['LoanAmount'])) # It gives 136 \n",
    "test_data.LoanAmount.fillna(136 ,inplace = True) # So, filled NA values with 146\n",
    "\n",
    "avgnum = np.round(np.mean(test_data['Credit_History'])) # It gives 1\n",
    "test_data.Credit_History.fillna(1 ,inplace = True) # So, filled NA values with 1\n",
    "\n",
    "test_data['Property_Area'] = number.fit_transform(test_data['Property_Area'].astype('str'))\n",
    "test_data['outcome'] = number.fit_transform(test_data['outcome'].astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Plots\"></a>\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:#000000\">\n",
    "# Plot and Graphs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to make some graphs to see the visualization of train dataset. For this I am going to used seaborn package, that is a is a Python visualization library based on matplotlib.[More Info.](https://seaborn.pydata.org/introduction.html#introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up the style and grid style of seaborn graphs\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "# palette=pkmn_type_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"label label-default\">Heatmap</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations Using Heatmap\n",
    "corr = train_data.corr()\n",
    "sns.heatmap(corr, annot=True, fmt=\"0.2f\"); # annot:write the data value in each cell | fmt:String formatting code d=decimal f=float  \n",
    "plt.xticks(rotation=-90) # Heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scale shows Pearson Coefficient (-1 to 1), the values near to 1 or -1 have high correlation. <br>\n",
    "The heatmap clearly shows that there is very less corelations between most of the variables and target variable **Loan_Status**. But **Credit_History** has a good corelation with **Loan_Status**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"label label-default\">Distribution Plot (a.k.a. Histogram)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution Plot (a.k.a. Histogram)\n",
    "sns.distplot(train_data.ApplicantIncome)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.factorplot(\"Gender\", \"Credit_History\", \"Education\", data=train_data, kind=\"bar\", palette=\"muted\", legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"label label-default\">Pair Plot</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pair Plot\n",
    "sns.pairplot(train_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"label label-default\">Boxplot</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot\n",
    "sns.boxplot(data=train_data, palette=\"deep\")\n",
    "sns.boxplot(x='Gender' , y='ApplicantIncome', data=train_data, palette=\"deep\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"label label-default\">Joint Distribution Plot</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint Distribution Plot\n",
    "sns.jointplot(x='Gender', y='Loan_Status', data=train_data)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"logreg\"></a>\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:#000000\">\n",
    "# === Logistic Regression ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### We will drop the Loan_ID coloumn as it is not required.  And build the model using all the variable first, to check the most significant variables (using p-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_new = train_data.drop(['Loan_ID'], 1)\n",
    "train_data_new.head(3) # 94.55% Accuracy | AUC = 92%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating Dataset\n",
    "test_data_new = test_data.drop(['Loan_ID'], 1)\n",
    "test_data_new.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking Train Dataset values in two variables\n",
    "X_train = train_data_new.ix[:,(0,1,2,3,4,5,6,7,8,9,10)]\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data_new.ix[:,11]\n",
    "y_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking Validate Dataset values as test data\n",
    "# The purpose of doing this is : Test Data does not have a \"Outcome\" variable \n",
    "# So, the predicted values will be compared with \"Outcome\" variable of Validate Data\n",
    "X_test = test_data_new.ix[:,(0,1,2,3,4,5,6,7,8,9,10)]\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_data_new.ix[:,11]\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"classreport_statsmod\"></a>\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:#000000\">\n",
    "# Classification Report using Stats Model\n",
    "### This is done to view the proper output of Logistic Regression. Because SciKit Learn package does not give this much of detailed output.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model = sm.Logit(y_train, sm.add_constant(X_train)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model.summary() # Gives the summary of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### When model with all the variable was generated it gives Accuracy of 94.55% and AUC of 92%. Also, it shows that the Married and Credit_History variable are only siginificant variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model.conf_int() # gives you idea for how robust the coefficients of the model are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(logit_model.params) # odds ratios # the exponential of each of the coefficients to generate the odds ratios.\n",
    "# All the odds ratio must be above 1, meaning that they are positively associated with <target_variable/dependent_variable>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span class=\"label label-primary\">Now, building a model using only Married and Credit History variables. <br>(As these variables are significant with Loan_Status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_new = train_data.drop(['Loan_ID', 'Gender','Dependents', 'Self_Employed', 'Education', 'Property_Area', 'ApplicantIncome', 'Loan_Amount_Term','CoapplicantIncome', 'LoanAmount' ], 1)\n",
    "train_data_new.head(3) # 95% Accuracy | ROC = 91%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_new = test_data.drop(['Loan_ID', 'Gender','Dependents', 'Self_Employed', 'Education', 'Property_Area', 'ApplicantIncome', 'Loan_Amount_Term','CoapplicantIncome', 'LoanAmount' ], 1)\n",
    "test_data_new.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking Train Dataset values in two variables\n",
    "X_train = train_data_new.ix[:,(0,1)]\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data_new.ix[:,2]\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking Validate Dataset values as test data\n",
    "# The purpose of doing this is : Test Data does not have a \"Outcome\" variable \n",
    "# So, the predicted values will be compared with \"Outcome\" variable of Validate Data\n",
    "X_test = test_data_new.ix[:,(0,1)]\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_data_new.ix[:,2]\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"classreport_scikit\"></a>\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:#000000\">\n",
    "## Classification Report using Sci Kit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(fit_intercept=True,C = 1e15)\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.get_params()\n",
    "logreg.decision_function(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Predicting the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The y_pred = logreg.predict(X_test) will give output as a class prediction (0 and 1) \n",
    "# for every observstion in a testing set, which will store in y_pred class.\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating R Sq.\n",
    "sklearn.metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification Report using Scikit Learn\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.score(X_test, y_test) # Exact Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ModPerformEva\"></a>\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:#000000\">\n",
    "### =======================================================================\n",
    "# Model Performance Evaluation \n",
    "### ======================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Classification Accuracy:__ Percentage of correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy or The correct classification of model in %\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Null accuracy: __ It is an accuracy that could be achieved by always predicting the most frequent class (most numbers of 0s or 1s). It is important to compare Classification accuracy with Null Accuracy \n",
    "**_Whichever, 0s or 1s is maximum is the NULL accuracy_**\n",
    "\n",
    "* This answer the question of _\"if my model wants to predict the predominant (main/chief/principal) class 100% of the time, How often would it be correct?_\n",
    "* Its like dummy model which would be correct the maximum percentage of the time. \n",
    "* In this case **79.01%** of the time the model will predict Loan Outcome as 1 (Yes). \n",
    "* Now, this is not a geniun model but it gives a baseline to predict out logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts() # examine the class distribution of the testing set (using a Pandas Series method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean() # calculate the percentage of ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - y_test.mean() # calculate the percentage of zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(y_test.mean(), 1 - y_test.mean()) # calculates null accuracy (for binary classification problems coded as 0/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts().head(1) / len(y_test) # calculates null accuracy (for multi-class classification problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(y_pred) # Count the elements of an numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Drawbacks of Classification accuracy :+1:\n",
    "* Classification accuracy is the easiest classification metric to understand. __But,__ it does not tell you the underlying distribution of response values (NULL accuracy tells you this).\n",
    "* And, it does not tell you what \"types\" of errors your classifier is making. ** _This problem can be solved by Confusion Matrix _ **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ConfMat\"></a>\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:#000000\">\n",
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Confusion Matrix Table describes the performance of a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confmat = confusion_matrix(y_test, y_pred) # IMPORTANT: first argument is true values, second is predicted values\n",
    "confmat # If you change the order of arguments the matrix will be reversed but no error will be raised.\n",
    "# So always use a fixed place of those arguments.\n",
    "# The result is telling us that we have 58+289=247 correct predictions\n",
    "# and 19+1=20 incorrect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* __True Positives (TP):__ we correctly predicted that they do have diabetes\n",
    "* __True Negatives (TN):__ we correctly predicted that they don't have diabetes\n",
    "* __False Positives (FP):__ we incorrectly predicted that they do have diabetes (a \"Type I error\")\n",
    "* __False Negatives (FN):__ we incorrectly predicted that they don't have diabetes (a \"Type II error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see the first 10 true and predicted responses\n",
    "print('True:', y_test.values[0:10])\n",
    "print('Pred:', y_pred[0:10])\n",
    "# Identify the four cases for the output generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# slice confusion matrix into four pieces and save it \n",
    "TP = confmat[1, 1]\n",
    "TN = confmat[0, 0]\n",
    "FP = confmat[0, 1]\n",
    "FN = confmat[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Accuracy:** Overall, how often is the classifier correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(TP + TN) / (TP + TN + FP + FN) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Classification Error (_\"Misclassification Rate\"_):** Overall, how often is the classifier incorrect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(FP + FN) / (TP + TN + FP + FN) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Sensitivity (_\"True Positive Rate\" or \"Recall\"_):** When the actual value is positive (1), how often is the prediction correct?\n",
    "* How \"sensitive\" is the classifier to detecting positive instances?\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specificity:** When the actual value is negative, how often is the prediction correct?\n",
    "* How \"specific\" (or \"selective\") is the classifier in predicting positive instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN / (TN + FP) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False Positive Rate:** When the actual value is negative, how often is the prediction incorrect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP / (TN + FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision:** When a positive value is predicted, how often is the prediction correct?\n",
    "* How \"precise\" is the classifier when predicting positive instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP / (TP + FP) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** F1 score is the harmonic mean of precision and sensitivity **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(2*TP) / ((2*TP) + FP + FN) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(y_test, y_pred)) # Classification Accuracy\n",
    "print(1 - metrics.accuracy_score(y_test, y_pred)) # Classification Error\n",
    "print(metrics.recall_score(y_test, y_pred)) # Sensitivity\n",
    "print(metrics.precision_score(y_test, y_pred)) # Precision\n",
    "# Specificity has no metric function in scikitlearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity and Specificity must be as high as possible. \n",
    "* In this model we can describe that our classifier is highly Sensitive and highly Specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"AdjClassThershold\"></a>\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:#000000\">\n",
    "# Adjusting the Classification Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 10 predicted responses\n",
    "logreg.predict(X_test)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 10 predicted probabilities of class membership\n",
    "logreg.predict_proba(X_test)[0:10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each row represents observation and the coloumn represents class 0 and 1\n",
    "* The sum of each row is 1\n",
    "* By default the classification threshold is set to 0.5 \n",
    "* So, out of 2 values in each row the value greater than or eaual to 0.5 is stored as '1'\n",
    "* **The value less than 0.5 is stored as '0'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (logreg.predict_proba(X_test)[0:10, 1]) # print the first 10 predicted probabilities for class 1\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:, 1] # And store the predicted probabilities for class 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Histogram_of_pred_prob\"></a>\n",
    "### Histogram of predicted probabilities for class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If kernel density estimate (KDE) is set to TRUE then it shows density at y-axis\n",
    "sns.distplot(y_pred_prob, color=\"red\", kde=False, rug=True) # In this plot number of counts are shown at y-axis\n",
    "plt.xlim(0, 1)\n",
    "plt.title('Histogram of predicted probabilities', fontsize=17, fontweight='bold')\n",
    "plt.xlabel('Predicted probability of diabetes', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From this histogram we can see these probablities varies between **0 to 1**. And most of the points are above 0.6\n",
    "* As we can clearly see that probability of ** 0.82 ** has the highst frequency  \n",
    "* This states that majority of prediction (of class 1) occured are above 0.5 **[Default Threshold = 0.5]**\n",
    "* If majority of prediction occured were below 0.5 then we might change our threshold value **(We can say class - 1 is rarely predicted)**\n",
    "* ** But in this case we can say that class - 1 frequently predicted **\n",
    "\n",
    "> * You can adjust Sensitivity and Specificity by setting threshold value\n",
    "> * Sensitivity and specificity have an inverse relationship\n",
    "> * Lower the cutoff higher will be the Sensitivity\n",
    "> * Higher the cutoff higher will be the Specificity\n",
    "> * So, depending on your business requirments you can increast or decrease thershold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"DecThreshold\"></a>\n",
    " <div class=\"alert alert-block alert-info\" style=\"color:#000000\">\n",
    " ** =====================================================================================================================**\n",
    " ## Decreasing the threshold in order to increase the sensitivity of the classifier  \n",
    " ** [Optional for this Project] **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict loan outcome if the predicted probability is greater than 0.1\n",
    "from sklearn.preprocessing import binarize\n",
    "y_pred_class = binarize([y_pred_prob], 0.1)[0] # Its a 2D numpy array and we will slice only first dimension\n",
    "\n",
    "y_pred_prob[0:10] # print the first 10 predicted probabilities\n",
    "y_pred_class[0:10] # print the first 10 predicted classes with the lower threshold\n",
    "print(confmat) # previous confusion matrix (default threshold of 0.5)\n",
    "metrics.confusion_matrix(y_test, y_pred_class) # new confusion matrix (threshold of 0.1)\n",
    "\n",
    "290 / (290 + 0) # sensitivity has increased to 1 (used to be 0.99655172)\n",
    "24 / (24 + 53) # specificity has decreased to 0.31168831 (used to be 0.7532467)\n",
    "# ================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <div class=\"alert alert-block alert-info\" style=\"color:#000000\">\n",
    " ** =====================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rocauc\"></a>\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:#000000\">\n",
    "# ROC Curves and Area Under the Curve (AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It would be nice if we can see sensitivity and specificity are affected by various thresholds\n",
    "* Without actually changing the threshold manually. (But if we change the threshold we have do the above process)\n",
    "* We can do this by ROC curve. **[Receiver operating characteristic]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: first argument is true values, second argument is predicted probabilities\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n",
    "auc = metrics.auc(fpr, tpr) \n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Higher the AUC value better is the Classifier\n",
    "* It is single number summary as a performance of classifier (alternative to accuract=y score)\n",
    "* If you randomly chose one positive and one negative observation, AUC represents the \n",
    "* likelihood that your classifier will assign a higher predicted probability to the positive observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr, color='red', lw=2, label='ROC Curve (Area = %0.2f)' % auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title('ROC Curve for Default Loan Classifier', fontsize=17, fontweight='bold')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"False Positive Rates : \", fpr) # Increasing false positive rates\n",
    "print(\"True Positive Rates : \",tpr) # Increasing true positive rates\n",
    "print(\"Thresholds : \",thresholds) # Decreasing thresholds on the deecision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The curve tells you that if you want to achave Senssitive of 0.90 then \n",
    "* You have to accept the specificity of 0.20\n",
    "* ROC curve can help you to visually choose a threshold that balances sensitivity and specificity\n",
    "* But you can't actually see the thresholds used to generate the curve on the ROC curve itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"label label-primary\"> A function that accepts a threshold and prints sensitivity and specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Refer to graph of [Histogram of Predicted robabilities](#Histogram_of_pred_prob) while studying the values of Sensitivity and Specificity.\n",
    "* The graph is having 1-Specificity on the X-axis, which is converted to Specificity in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_threshold(threshold):\n",
    "    print('Sensitivity:', tpr[thresholds > threshold][-1])\n",
    "    print('Specificity:', 1 - fpr[thresholds > threshold][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that, when we set cutoff/threshold of 0.5 we get the **Sensitivity=0.99** and **Specificity: 0.75**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_threshold(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But when we set the cutoff to 0.01 we get the **Sensitivity=1** and **Specificity: 0**. This is because all the probabilities are above 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_threshold(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** So depending on your business requirment you have to adjust Sensitivity and Specificity **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ExportVals\"></a>\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:#000000\">\n",
    "## Exporting predicted values in Validate Dataset File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['Predicted Loan Status'] = y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_data.to_csv('F:/Lectures/Data Science/iMarticus/Python/Project/Project 5-6 Case Study on Credit Risk/Predicted_Data.csv', index=False, encoding='utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Conclusion\"></a>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Confusion matrix advantages:**\n",
    "* Allows you to calculate a variety of metrics\n",
    "* Useful for multi-class problems (more than two response classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "** ROC/AUC advantages:**\n",
    "* Does not require you to set a classification threshold\n",
    "* Still useful when there is high class imbalance\n",
    "* However, multi-class problems it is difficult to identify threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#head)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
